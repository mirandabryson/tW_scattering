{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.accumulator import AccumulatorABC\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from coffea.btag_tools import BTagScaleFactor\n",
    "from coffea import hist\n",
    "import pandas as pd\n",
    "import uproot_methods\n",
    "import uproot\n",
    "import awkward\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from Tools.config_helpers import *\n",
    "from Tools.helpers import mergeArray, mt, get_scheduler_address\n",
    "\n",
    "from Tools.objects import Collections\n",
    "from Tools.cutflow import Cutflow\n",
    "\n",
    "# This just tells matplotlib not to open any\n",
    "# interactive windows.\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_flatten(val): \n",
    "    try:\n",
    "        return val.pad(1, clip=True).fillna(0.).flatten()#.reshape(-1, 1)\n",
    "    except AttributeError:\n",
    "        return val.flatten()\n",
    "\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "#from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables... to avoid making bugs!!!\n",
    "processesList = ['DYJets', 'TTJets', 'ttW', 'ttZ', 'Data']\n",
    "linesList= ['triggers', 'dilepton sf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysisProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Processor used for running the analysis\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        ## load b-tag SFs\n",
    "        #self.btag_sf = BTagScaleFactor(os.path.expandvars(\"$TWHOME/data/DeepCSV_102XSF_V1.btag.csv.gz\", \"reshape\")\n",
    "\n",
    "        ## load the NN\n",
    "        #self.model = load_model('../ML/data/training.h5')\n",
    "        #self.stds  = pd.read_json('../ML/data/stds.json').squeeze()\n",
    "        #self.means = pd.read_json('../ML/data/means.json').squeeze()\n",
    "        \n",
    "        # we can use a large number of bins and rebin later\n",
    "        dataset_axis        = hist.Cat(\"dataset\",   \"Primary dataset\")\n",
    "        pt_axis             = hist.Bin(\"pt\",        r\"$p_{T}$ (GeV)\", 1000, 0, 1000)\n",
    "        p_axis              = hist.Bin(\"p\",         r\"$p$ (GeV)\", 1000, 0, 2500)\n",
    "        ht_axis             = hist.Bin(\"ht\",        r\"$H_{T}$ (GeV)\", 500, 0, 5000)\n",
    "        mass_axis           = hist.Bin(\"mass\",      r\"Mass (GeV)\", 1000, 0, 2000)\n",
    "        eta_axis            = hist.Bin(\"eta\",       r\"$\\eta$\", 60, -5.5, 5.5)\n",
    "        delta_axis          = hist.Bin(\"delta\",     r\"$\\delta$\", 100,0,10 )\n",
    "        multiplicity_axis   = hist.Bin(\"multiplicity\",         r\"N\", 20, -0.5, 19.5)\n",
    "        norm_axis           = hist.Bin(\"norm\",         r\"N\", 25, 0, 1)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            \"Z_mass\":                  hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"MET_pt_baseline\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"HT_baseline\" :              hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            #\"mtb_min_baseline\" :         hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"MET_pt\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"HT\" :              hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            #\"mtb_min\" :         hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"MET_pt_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"HT_CR\" :           hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            #\"mtb_min_CR\" :      hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            #\"lead_AK8_pt\" :     hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"W_pt\" :            hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"H_pt\" :            hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            #\"W_eta\" :           hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            #\"H_eta\" :           hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "#             \"met_CR\":           hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"met_Higgs_CR\":     hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"met_W_CR\":         hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"met_Higgs_W_CR\":   hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \n",
    "#             \"ht_CR\":            hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "#             \"ht_Higgs_CR\":      hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "#             \"ht_W_CR\":          hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "#             \"ht_Higgs_W_CR\":    hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \n",
    "#             \"N_AK8_CR\" :        hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "#             \"W_pt_CR\" :         hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"H_pt_CR\" :         hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"W_eta_CR\" :        hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "#             \"H_eta_CR\" :        hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "#             \"N_AK8_Higgs_CR\" :  hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "#             \"W_pt_Higgs_CR\" :   hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"H_pt_Higgs_CR\" :   hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"W_eta_Higgs_CR\" :  hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "#             \"H_eta_Higgs_CR\" :  hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "#             \"N_AK8_W_CR\" :      hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "#             \"W_pt_W_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"H_pt_W_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"W_eta_W_CR\" :      hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "#             \"H_eta_W_CR\" :      hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "#             \"N_AK8_Higgs_W_CR\" :hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "#             \"W_pt_Higgs_W_CR\" : hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"H_pt_Higgs_W_CR\" : hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "#             \"W_eta_Higgs_W_CR\" :hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "#             \"H_eta_Higgs_W_CR\" :hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "\n",
    "            #\"N_b\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_AK4\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_AK8\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_H\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            #\"N_W\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \n",
    "            #\"WH_deltaPhi\":      hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"WH_deltaR\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"bb_deltaPhi\":      hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"bb_deltaR\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"min_dphiJetMet4\":  hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"dphiDiJet\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            #\"dphiDiFatJet\":     hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \n",
    "#             'mC750_l1':         processor.defaultdict_accumulator(int),\n",
    "#             'WJets':            processor.defaultdict_accumulator(int),\n",
    "#             'QCD':              processor.defaultdict_accumulator(int),\n",
    "            'TTJets':           processor.defaultdict_accumulator(int),\n",
    "            'DYJets':           processor.defaultdict_accumulator(int),\n",
    "#             'ZNuNu':            processor.defaultdict_accumulator(int),\n",
    "#             'ST':               processor.defaultdict_accumulator(int),\n",
    "#             'ST_tW':            processor.defaultdict_accumulator(int),\n",
    "#             'ST_tChannel':      processor.defaultdict_accumulator(int),\n",
    "#             'ST_sChannel':      processor.defaultdict_accumulator(int),\n",
    "            'ttW':              processor.defaultdict_accumulator(int),\n",
    "            'ttZ':              processor.defaultdict_accumulator(int),\n",
    "#             'WW':               processor.defaultdict_accumulator(int),\n",
    "#             'WZ/ZZ':            processor.defaultdict_accumulator(int),\n",
    "#             'LL':               processor.defaultdict_accumulator(int),\n",
    "            'Data':             processor.defaultdict_accumulator(int),\n",
    "            'totalEvents':      processor.defaultdict_accumulator(int),\n",
    "#             'test1':            processor.defaultdict_accumulator(float),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \"\"\"\n",
    "        Processing function. This is where the actual analysis happens.\n",
    "        \"\"\"\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df[\"dataset\"]\n",
    "        cfg = loadConfig()\n",
    "        \n",
    "        ############## MET ##############\n",
    "        \n",
    "        met_pt  = df[\"MET_pt\"]\n",
    "        met_phi = df[\"MET_phi\"]\n",
    "        \n",
    "        ############## LEPTONS ############## \n",
    "        \n",
    "        muon = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt = df['Muon_pt'].content,\n",
    "            eta = df['Muon_eta'].content,\n",
    "            phi = df['Muon_phi'].content,\n",
    "            mass = df['Muon_mass'].content,\n",
    "            miniPFRelIso_all=df['Muon_miniPFRelIso_all'].content,\n",
    "            looseId =df['Muon_looseId'].content\n",
    "            )\n",
    "        muon = muon[(muon.pt > 10) & (abs(muon.eta) < 2.4) & (muon.looseId) & (muon.miniPFRelIso_all < 0.2)]\n",
    "        #muon = Collections(df, \"Muon\", \"tightTTH\").get() # this needs a fix for DASK\n",
    "        \n",
    "        electron = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nElectron'],\n",
    "            pt = df['Electron_pt'].content,\n",
    "            eta = df['Electron_eta'].content,\n",
    "            phi = df['Electron_phi'].content,\n",
    "            mass = df['Electron_mass'].content,\n",
    "            miniPFRelIso_all=df['Electron_miniPFRelIso_all'].content,\n",
    "            cutBased=df['Electron_cutBased'].content\n",
    "            )\n",
    "        electron = electron[(electron.pt>10) & (abs(electron.eta) < 2.4) & (electron.miniPFRelIso_all < 0.1) &  (electron.cutBased >= 1)]\n",
    "        #electron = Collections(df, \"Electron\", \"tightTTH\").get() # this needs a fix for DASK\n",
    "\n",
    "        high_pt_e = electron[electron.pt.argsort(ascending=False)][:,:2]\n",
    "        ee = high_pt_e.choose(2)\n",
    "        \n",
    "        high_pt_m = muon[muon.pt.argsort(ascending=False)][:,:2]\n",
    "        mm = high_pt_m.choose(2)\n",
    "        mm_mass = mm.mass\n",
    "\n",
    "\n",
    "        \n",
    "        ############## FATJETS ##############\n",
    "        \n",
    "        fatjet = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nFatJet'],\n",
    "            pt = df['FatJet_pt'].content,\n",
    "            eta = df['FatJet_eta'].content,\n",
    "            phi = df['FatJet_phi'].content,\n",
    "            mass = df['FatJet_mass'].content,\n",
    "            msoftdrop = df[\"FatJet_msoftdrop\"].content,  \n",
    "            deepTagMD_HbbvsQCD = df['FatJet_deepTagMD_HbbvsQCD'].content, \n",
    "            deepTagMD_WvsQCD = df['FatJet_deepTagMD_WvsQCD'].content, \n",
    "            deepTag_WvsQCD = df['FatJet_deepTag_WvsQCD'].content\n",
    "            \n",
    "        )\n",
    "        \n",
    "        leadingFatJets = fatjet[:,:2]\n",
    "        difatjet = leadingFatJets.choose(2)\n",
    "        dphiDiFatJet = np.arccos(np.cos(difatjet.i0.phi-difatjet.i1.phi))\n",
    "        \n",
    "        htag = fatjet[((fatjet.pt > 200) & (fatjet.deepTagMD_HbbvsQCD > 0.8365))]\n",
    "        htag_hard = fatjet[((fatjet.pt > 300) & (fatjet.deepTagMD_HbbvsQCD > 0.8365))]\n",
    "        \n",
    "        lead_htag = htag[htag.pt.argmax()]\n",
    "        \n",
    "        wtag = fatjet[((fatjet.pt > 200) & (fatjet.deepTagMD_HbbvsQCD < 0.8365) & (fatjet.deepTag_WvsQCD > 0.918))]\n",
    "        wtag_hard = fatjet[((fatjet.pt > 300) & (fatjet.deepTagMD_HbbvsQCD < 0.8365) & (fatjet.deepTag_WvsQCD > 0.918))]\n",
    "        \n",
    "        lead_wtag = wtag[wtag.pt.argmax()]\n",
    "        \n",
    "        wh = lead_htag.cross(lead_wtag)\n",
    "        wh_deltaPhi = np.arccos(wh.i0.phi - wh.i1.phi)\n",
    "        wh_deltaR = wh.i0.p4.delta_r(wh.i1.p4)\n",
    "        \n",
    "        ############## JETS ##############\n",
    "        \n",
    "        jet = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt = df['Jet_pt'].content,\n",
    "            eta = df['Jet_eta'].content,\n",
    "            phi = df['Jet_phi'].content,\n",
    "            mass = df['Jet_mass'].content,\n",
    "            jetId = df['Jet_jetId'].content, # https://twiki.cern.ch/twiki/bin/view/CMS/JetID\n",
    "            #puId = df['Jet_puId'].content, # https://twiki.cern.ch/twiki/bin/viewauth/CMS/PileupJetID\n",
    "            btagDeepB = df['Jet_btagDeepB'].content, # https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation102X\n",
    "            #deepJet = df['Jet_'].content # not there yet?\n",
    "        )\n",
    "        \n",
    "        skimjet   = jet[(jet.pt>30) & (abs(jet.eta)<2.4)]\n",
    "        jet       = jet[(jet.pt>30) & (jet.jetId>1) & (abs(jet.eta)<2.4)]\n",
    "        jet       = jet[~jet.match(muon, deltaRCut=0.4)] # remove jets that overlap with muons\n",
    "        jet       = jet[~jet.match(electron, deltaRCut=0.4)] # remove jets that overlap with electrons\n",
    "        jet       = jet[~jet.match(fatjet, deltaRCut=1.2)] # remove AK4 jets that overlap with AK8 jets\n",
    "        jet       = jet[jet.pt.argsort(ascending=False)] # sort the jets\n",
    "        btag      = jet[(jet.btagDeepB>0.4184)]\n",
    "        light     = jet[(jet.btagDeepB<0.4184)]\n",
    "        \n",
    "        ## Get the leading b-jets\n",
    "        high_score_btag = jet[jet.btagDeepB.argsort(ascending=False)][:,:2]\n",
    "        \n",
    "        leading_jet    = jet[jet.pt.argmax()]\n",
    "        leading_b      = btag[btag.pt.argmax()]\n",
    "        \n",
    "        bb = high_score_btag.choose(2)\n",
    "        bb_deltaPhi = np.arccos(np.cos(bb.i0.phi-bb.i1.phi))\n",
    "        bb_deltaR = bb.i0.p4.delta_r(bb.i1.p4)\n",
    "        \n",
    "        mtb = mt(btag.pt, btag.phi, met_pt, met_phi)\n",
    "        min_mtb = mtb.min()\n",
    "        mth = mt(htag.pt, htag.phi, met_pt, met_phi)\n",
    "\n",
    "        ############## OTHER VARIABLES ##############\n",
    "        \n",
    "        ht = jet.pt.sum()\n",
    "        \n",
    "        min_dphiJetMet4 = np.arccos(np.cos(jet[:,:4].phi-met_phi)).min()\n",
    "        \n",
    "        leadingJets = jet[:,:2]\n",
    "        dijet = leadingJets.choose(2)\n",
    "        dphiDiJet = np.arccos(np.cos(dijet.i0.phi-dijet.i1.phi))\n",
    "        \n",
    "        min_dphiFatJetMet4 = np.arccos(np.cos(fatjet[:,:4].phi-met_phi)).min()\n",
    "\n",
    "        ############## FILTERS ##############\n",
    "        \n",
    "        good_vertices = df[\"Flag_goodVertices\"]\n",
    "        tighthalo = df[\"Flag_globalSuperTightHalo2016Filter\"]\n",
    "        noise_filter = df[\"Flag_HBHENoiseFilter\"]\n",
    "        noise_isofilter = df[\"Flag_HBHENoiseIsoFilter\"]\n",
    "        ecal_deadcell = df[\"Flag_EcalDeadCellTriggerPrimitiveFilter\"]\n",
    "        bad_pfmuon = df[\"Flag_BadPFMuonFilter\"]\n",
    "        ee_badsc = df[\"Flag_eeBadScFilter\"]\n",
    "       \n",
    "        ############## TRIGGERS ##############\n",
    "        hlt_mu17 = df[\"HLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL\"]\n",
    "        hlt_mu17_dz = df[\"HLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL_DZ\"]\n",
    "        hlt_mu50 = df[\"HLT_Mu50\"]\n",
    "        hlt_mu55 = df[\"HLT_Mu55\"]\n",
    "        \n",
    "        dimuon_tsel = (hlt_mu17 == 1) & (hlt_mu17_dz == 1) & (hlt_mu50 == 1) & (hlt_mu55 ==1)\n",
    "        \n",
    "#         hlt_pfmet_250 = df[\"HLT_PFMET250_HBHECleaned\"]\n",
    "#         hlt_pfmet_300 = df[\"HLT_PFMET300_HBHECleaned\"]\n",
    "#         hlt_pfmet1_200 = df[\"HLT_PFMETTypeOne200_HBHE_BeamHaloCleaned\"]\n",
    "#         hlt_pfmet_mht = df[\"HLT_PFMET120_PFMHT120_IDTight_PFHT60\"]\n",
    "#         hlt_pfmetNoMu_mhtNoMu = df[\"HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\"]\n",
    "        \n",
    "#         met_fsel = (good_vertices == 1) & (tighthalo == 1) & (noise_filter == 1) & (noise_isofilter == 1) & (ecal_deadcell == 1) & (bad_pfmuon == 1) & (ee_badsc == 1) \n",
    "#         met_tsel = (hlt_pfmet_250 == 1) | (hlt_pfmet_300 == 1) | (hlt_pfmet1_200 == 1) | (hlt_pfmet_mht == 1) | (hlt_pfmetNoMu_mhtNoMu == 1)\n",
    "                \n",
    "#         wtag_sel = ( wtag.counts>0 & (abs(wtag.msoftdrop-80)<30).any())\n",
    "#         htag_sel = ( htag.counts>0 & (abs(htag.msoftdrop-125)<25).any())\n",
    "#         stitchVar = 1 if dataset=='Data' else df[\"stitch\"]\n",
    "        \n",
    "        ############## SELECTIONS ##############\n",
    "        \n",
    "        dilep_sel = ((electron.counts+muon.counts)==2)\n",
    "        dilep_sf_sel = dilep_sel & ((electron.counts ==2 )|(muon.counts == 2))\n",
    "        dilep_of_sel = dilep_sel & ((electron.counts ==1 )|(muon.counts == 1))\n",
    "        \n",
    "        ############## CUTFLOW ##############\n",
    "        \n",
    "        output['totalEvents']['all'] += len(df['weight'])\n",
    "        processes = processesList\n",
    "        weight = np.ones(len(df['weight'])) if dataset=='Data' else df['weight']\n",
    "        lumi = 1 if dataset=='Data' else 137\n",
    "        fullweight = weight * lumi\n",
    "        \n",
    "        cutflow = Cutflow(output, df, cfg, processes, weight=fullweight)\n",
    "        cutflow.addRow( 'triggers',   (dimuon_tsel) )        \n",
    "        cutflow.addRow('dilepton sf', (dilep_sf_sel))\n",
    "#         cutflow.addRow( 'good_vertices',   (good_vertices==1) )\n",
    "#         cutflow.addRow( 'tighthalo',   (tighthalo==1) )\n",
    "#         cutflow.addRow( 'noise_filter',   (noise_filter==1) )\n",
    "#         cutflow.addRow( 'noise_isofilter',   (noise_isofilter==1) )\n",
    "#         cutflow.addRow( 'ecal_deadcell',   (ecal_deadcell==1) )\n",
    "#         cutflow.addRow( 'bad_pfmuon',   (bad_pfmuon==1) )\n",
    "#         cutflow.addRow( 'ee_badsc',   (ee_badsc==1) )\n",
    "\n",
    "#         cutflow.addRow( 'triggers',   (met_tsel) )\n",
    "        \n",
    "#         cutflow.addRow( 'stitch',   (stitchVar ==1) )\n",
    "        \n",
    "#         cutflow.addRow( 'skim',   ((met_pt>200) & (skimjet.counts>1)) )\n",
    "#         cutflow.addRow( 'Exactly 1 e or mu',   ((electron.counts+muon.counts)==1) )\n",
    "#         cutflow.addRow( 'MET>250',     (met_pt>250) )\n",
    "        \n",
    "#         baseline = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "#         cutflow.addRow( 'N_fatjet>1',      (fatjet.counts>1) )\n",
    "#         cutflow.addRow( 'min_dphiFatJetMet4', (min_dphiFatJetMet4>0.5))\n",
    "#         cutflow.addRow( 'dphiDiFatJet', (dphiDiFatJet<2.5).all() ) # by using .all() I do not implicitely cut on the number of fat jets\n",
    "#         cutflow.addRow( 'minmth>200',   (mth.min()>200) )\n",
    "#         cutflow.addRow( 'njet veto',     (jet.counts<2))\n",
    "\n",
    "#         vetoQCD = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "#         cutflow.addRow( 'N_wtag>0',     (wtag_sel), cumulative=False)\n",
    "        \n",
    "#         wtag_selection = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "#         cutflow.addRow( 'N_htag>0',     (htag_sel), cumulative=False)\n",
    "\n",
    "#         htag_selection = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "#         cutflow.addRow( 'N_htag>0, N_wtag>0',     (htag_sel & wtag_sel))\n",
    "\n",
    "#         signal_selection = cutflow.selection\n",
    "        \n",
    "        ############## HISTOGRAMS ##############\n",
    "        \n",
    "        output['Z_mass'].fill(dataset=dataset,mass=mm_mass.flatten(),weight=fullweight)\n",
    "#         output['met_CR'].fill(dataset=dataset, pt=met_pt[vetoQCD].flatten(), weight=fullweight[vetoQCD])\n",
    "#         output['met_W_CR'].fill(dataset=dataset, pt=met_pt[vetoQCD & wtag_sel].flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "#         output['met_Higgs_CR'].fill(dataset=dataset, pt=met_pt[vetoQCD & htag_sel].flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "#         output['met_Higgs_W_CR'].fill(dataset=dataset, pt=met_pt[signal_selection].flatten(), weight=fullweight[signal_selection])\n",
    "\n",
    "#         output['ht_CR'].fill(dataset=dataset, ht=ht[vetoQCD].flatten(), weight=fullweight[vetoQCD])\n",
    "#         output['ht_W_CR'].fill(dataset=dataset, ht=ht[vetoQCD & wtag_sel].flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "#         output['ht_Higgs_CR'].fill(dataset=dataset, ht=ht[vetoQCD & htag_sel].flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "#         output['ht_Higgs_W_CR'].fill(dataset=dataset, ht=ht[signal_selection].flatten(), weight=fullweight[signal_selection])\n",
    "        \n",
    "#         output['N_AK8_CR'].fill(dataset=dataset, multiplicity=fatjet[vetoQCD].counts, weight=fullweight[vetoQCD])\n",
    "#         #output['W_pt_CR'].fill(dataset=dataset, pt=lead_wtag[vetoQCD].pt.flatten(), weight=fullweight[vetoQCD])\n",
    "        #output['H_pt_CR'].fill(dataset=dataset, pt=lead_htag[vetoQCD].pt.flatten(), weight=fullweight[vetoQCD])\n",
    "        #output['W_eta_CR'].fill(dataset=dataset, eta=lead_wtag[vetoQCD].eta.flatten(), weight=fullweight[vetoQCD])\n",
    "        #output['H_eta_CR'].fill(dataset=dataset, eta=lead_htag[vetoQCD].eta.flatten(), weight=fullweight[vetoQCD])\n",
    "\n",
    "#         output['N_AK8_W_CR'].fill(dataset=dataset, multiplicity=fatjet[vetoQCD & wtag_sel].counts, weight=fullweight[vetoQCD & wtag_sel])\n",
    "#         output['W_pt_W_CR'].fill(dataset=dataset, pt=lead_wtag[vetoQCD & wtag_sel].pt.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "#         #output['H_pt_W_CR'].fill(dataset=dataset, pt=lead_htag[vetoQCD & wtag_sel].pt.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        #output['W_eta_W_CR'].fill(dataset=dataset, eta=lead_wtag[vetoQCD & wtag_sel].eta.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "        #output['H_eta_W_CR'].fill(dataset=dataset, eta=lead_htag[vetoQCD & wtag_sel].eta.flatten(), weight=fullweight[vetoQCD & wtag_sel])\n",
    "\n",
    "        #output['N_AK8_Higgs_CR'].fill(dataset=dataset, multiplicity=fatjet[vetoQCD & htag_sel].counts, weight=fullweight[vetoQCD & htag_sel])\n",
    "        #output['W_pt_Higgs_CR'].fill(dataset=dataset, pt=lead_wtag[vetoQCD & htag_sel].pt.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        #output['H_pt_Higgs_CR'].fill(dataset=dataset, pt=lead_htag[vetoQCD & htag_sel].pt.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        #output['W_eta_Higgs_CR'].fill(dataset=dataset, eta=lead_wtag[vetoQCD & htag_sel].eta.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "        #output['H_eta_Higgs_CR'].fill(dataset=dataset, eta=lead_htag[vetoQCD & htag_sel].eta.flatten(), weight=fullweight[vetoQCD & htag_sel])\n",
    "\n",
    "#         output['N_AK8_Higgs_W_CR'].fill(dataset=dataset, multiplicity=fatjet[signal_selection].counts, weight=fullweight[signal_selection])\n",
    "#         output['W_pt_Higgs_W_CR'].fill(dataset=dataset, pt=lead_wtag[signal_selection].pt.flatten(), weight=fullweight[signal_selection])\n",
    "#         output['H_pt_Higgs_W_CR'].fill(dataset=dataset, pt=lead_htag[signal_selection].pt.flatten(), weight=fullweight[signal_selection])\n",
    "#         output['W_eta_Higgs_W_CR'].fill(dataset=dataset, eta=lead_wtag[signal_selection].eta.flatten(), weight=fullweight[signal_selection])\n",
    "#         output['H_eta_Higgs_W_CR'].fill(dataset=dataset, eta=lead_htag[signal_selection].eta.flatten(), weight=fullweight[signal_selection])\n",
    "\n",
    "                \n",
    "#         output['MET_pt_baseline'].fill(dataset=dataset, pt=met_pt[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "#         output['HT_baseline'].fill(dataset=dataset, ht=ht[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "#         output['mtb_min_baseline'].fill(dataset=dataset, mass=mtb[baseline].min().flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "#         output['MET_pt'].fill(dataset=dataset, pt=met_pt[vetoQCD].flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "#         output['HT'].fill(dataset=dataset, ht=ht[vetoQCD].flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "#         output['mtb_min'].fill(dataset=dataset, mass=mtb[vetoQCD].min().flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        \n",
    "#         ## N jet and N b without selections on those\n",
    "#         output['N_AK4'].fill(dataset=dataset, multiplicity=jet[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])\n",
    "#         output['N_b'].fill(dataset=dataset, multiplicity=btag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "#         output['N_W'].fill(dataset=dataset, multiplicity=htag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "#         output['N_H'].fill(dataset=dataset, multiplicity=wtag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "#         output['N_AK8'].fill(dataset=dataset, multiplicity=fatjet[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "\n",
    "#         #output['bb_deltaPhi'].fill(dataset=dataset, delta=bb_deltaPhi[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "#         #output['bb_deltaR'].fill(dataset=dataset, delta=bb_deltaR[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "#         output['min_dphiJetMet4'].fill(dataset=dataset, delta=min_dphiJetMet4[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "#         output['dphiDiJet'].fill(dataset=dataset, delta=dphiDiJet[baseline].min().flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "#         ## Higgs and W pt\n",
    "#         output['lead_AK8_pt'].fill(dataset=dataset, pt=fatjet[(baseline & (fatjet.counts>0))].pt.max().flatten(), weight=df['weight'][(baseline & (fatjet.counts>0))]*cfg['lumi'])\n",
    "#         output['dphiDiFatJet'].fill(dataset=dataset, delta=dphiDiFatJet[(baseline & (fatjet.counts>1))].min().flatten(), weight=df['weight'][(baseline & (fatjet.counts>1))]*cfg['lumi'])\n",
    "\n",
    "#         output['H_pt'].fill(dataset=dataset, pt=lead_htag[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "#         output['H_eta'].fill(dataset=dataset, eta=lead_htag[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "#         output['W_pt'].fill(dataset=dataset, pt=lead_wtag[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "#         output['W_eta'].fill(dataset=dataset, eta=lead_wtag[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "#         output['WH_deltaPhi'].fill(dataset=dataset, delta=wh_deltaPhi[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "#         output['WH_deltaR'].fill(dataset=dataset, delta=wh_deltaR[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "#         output['MET_pt_CR'].fill(dataset=dataset, pt=met_pt[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "#         output['HT_CR'].fill(dataset=dataset, ht=ht[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "#         output['mtb_min_CR'].fill(dataset=dataset, mass=mtb[event_selection].min().flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "runLocal = True\n",
    "\n",
    "\n",
    "if not runLocal:\n",
    "    # Get the scheduler from the dask_cluster notebook\n",
    "    from dask.distributed import Client, progress\n",
    "\n",
    "    c = Client('tcp://169.228.130.5:27879')\n",
    "\n",
    "    ## for dask\n",
    "    exe_args = {\n",
    "        'client': c,\n",
    "        #'savemetrics': True,\n",
    "    }\n",
    "    exe = processor.dask_executor\n",
    "    \n",
    "else:\n",
    "    ## for local\n",
    "    exe_args = {\n",
    "        'workers': 4,\n",
    "        'function_args': {'flatten': False}\n",
    "    }\n",
    "    exe = processor.futures_executor\n",
    "\n",
    "if not runLocal:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f9c1355ee447b5af1b6adaeffdbbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value='Processing'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mbryson/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/users/mbryson/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mbryson/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/users/mbryson/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "small = True\n",
    "\n",
    "tag = 'v0.2.4'\n",
    "\n",
    "\n",
    "fileset_all_2016   = {'Data': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/DoubleMuon*2016*/*')\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/DoubleMuon*2016*/*'),\n",
    "                'DYJets': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/DYJetsToLL*Summer16*/*')\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/DYJetsToLL*Summer16*/*'),\n",
    "                'TTJets': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/TTJets_DiLept_Tune*Summer16*/*')\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/TTJets_DiLept_Tune*Summer16*/*'),\n",
    "                'ttW': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/TTWJets*Summer16*/*')\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/TTWJets*Summer16*/*'),\n",
    "                'ttZ': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/ttZJets*Summer16*/*')\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/ttZJets*Summer16*/*')\n",
    "                 }\n",
    "\n",
    "fileset_all_2016_sm   = {'Data': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/DoubleMuon*2016*/*')[:2]\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/DoubleMuon*2016*/*')[:2],\n",
    "                'DYJets': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/DYJetsToLL*Summer16*/*')[:2]\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/DYJetsToLL*Summer16*/*')[:2],\n",
    "                'TTJets': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/TTJets_DiLept_Tune*Summer16*/*')[:2]\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/TTJets_DiLept_Tune*Summer16*/*')[:2],\n",
    "                'ttW': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/TTWJets*Summer16*/*')[:2]\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/TTWJets*Summer16*/*')[:2],\n",
    "                'ttZ': glob.glob('/hadoop/cms/store/user/mbryson/WH_hadronic/'+tag+'/ttZJets*Summer16*/*')[:2]\n",
    "                + glob.glob('/hadoop/cms/store/user/dspitzba/WH_hadronic/'+tag+'/ttZJets*Summer16*/*')[:2]\n",
    "                 }\n",
    "\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'WH_small' if small else 'WH'\n",
    "\n",
    "# histograms\n",
    "histograms = []\n",
    "histograms += ['Z_mass']\n",
    "\n",
    "# initialize cache\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cfg['caches'][cacheName]), serialized=True)\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "\n",
    "else:\n",
    "    # Run the processor\n",
    "\n",
    "    if small:\n",
    "        fileset = fileset_all_2016_sm\n",
    "        workers = 4\n",
    "    else:\n",
    "        fileset = fileset_all_2016\n",
    "        workers = 16\n",
    "    \n",
    "        \n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=analysisProcessor(),\n",
    "                                      executor=exe,\n",
    "                                      executor_args=exe_args,\n",
    "                                      chunksize=250000,\n",
    "                                      #chunksize=100000,\n",
    "                                     )\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutflow\n",
    "from Tools.helpers import getCutFlowTable\n",
    "\n",
    "processes = processesList\n",
    "lines     = ['entry']\n",
    "lines    += linesList\n",
    "df        = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiencies\n",
    "df = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=3, absolute=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.helpers import *\n",
    "bins = {\n",
    "    'Z_mass_coarse':   {'axis':'mass',   'overflow': 'over', 'bins': hist.Bin(\"mass\",  r\"Z Boson Mass (GeV)\", 1000, 0, 2000)},\n",
    "#     'N_AK4':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{AK4 jet}$', 6, -0.5, 5.5)},\n",
    "#     'N_AK4_SR':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{AK4 jet}$', 6, -0.5, 5.5)},\n",
    "#     'N_AK8':    {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{AK8 jet}$', 5, -0.5, 4.5)},\n",
    "#     'N_b':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{b-tag}$', 5, -0.5, 4.5)},\n",
    "#     'N_H':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{H-tag}$', 5, -0.5, 4.5)},\n",
    "#     'N_W':      {'axis': 'multiplicity',  'overflow':'over',  'bins': hist.Bin('multiplicity', r'$N_{W-tag}$', 5, -0.5, 4.5)},\n",
    "\n",
    "#     'MET_pt':   {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T}^{miss}\\ (GeV)$', 20, 0, 800)},\n",
    "#     'MET_ptCoarse':   {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T}^{miss}\\ (GeV)$', 5, 200, 700)},\n",
    "#     'HT':       {'axis': 'ht',      'overflow':'over',  'bins': hist.Bin('pt', r'$H_{T} (AK4 jets) \\ (GeV)$', 25, 0, 2000)},    \n",
    "#     'HT_Coarse':       {'axis': 'ht',      'overflow':'over',  'bins': hist.Bin('pt', r'$H_{T} (AK4 jets) \\ (GeV)$', 5, 0, 500)},    \n",
    "#     'W_pt':     {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T} (W-tag)$', 8, 200, 600)},\n",
    "#     'W_eta':    {'axis': 'eta',     'overflow':'over',  'bins': hist.Bin('eta', r'$\\eta (W-tag)$', 15, -5.5, 5.5)},\n",
    "#     'H_pt':     {'axis': 'pt',      'overflow':'over',  'bins': hist.Bin('pt', r'$p_{T} (H-tag)$', 8, 200, 600)},\n",
    "#     'H_eta':    {'axis': 'eta',     'overflow':'over',  'bins': hist.Bin('eta', r'$\\eta (H-tag)$', 15, -5.5, 5.5)},\n",
    "\n",
    "#     'dphiDiFatJet': {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (AK8)$', 30, 0, 3)},\n",
    "#     'dphiDiJet':    {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (AK4)$', 30, 0, 3)},\n",
    "#     'WH_deltaPhi':  {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (WH)$', 6, 0, 3)},\n",
    "#     'WH_deltaR':    {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta R (WH)$', 10, 0, 5)},\n",
    "#     'bb_deltaPhi':  {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (bb)$', 30, 0, 3)},\n",
    "#     'bb_deltaR':    {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta R (bb)$', 10, 0, 5)},\n",
    "#     'min_dphiJetMet4': {'axis': 'delta',          'overflow':'over',  'bins': hist.Bin('delta', r'$\\Delta \\phi (j, p_{T}^{miss})$', 30, 0, 3)},\n",
    "        \n",
    "#     'mtb_min':      {'axis': 'mass',  'overflow':'over',  'bins': hist.Bin('pt', r'$min M_{T} (b, p_{T}^{miss}) \\ (GeV)$', 25, 0, 500)},\n",
    "#     'lead_AK8_pt':  {'axis': 'pt',    'overflow':'over',  'bins': hist.Bin('pt', r'$p{T} (lead. AK8) \\ (GeV)$', 20, 0, 1000)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.helpers import *\n",
    "\n",
    "def saveFig( fig, ax, rax, path, name, scale='linear', shape=False, y_max=-1 ):\n",
    "    outdir = os.path.join(path,scale)\n",
    "    finalizePlotDir(outdir)\n",
    "    ax.set_yscale(scale)\n",
    "    ax.set_ylabel('Events')\n",
    "\n",
    "    if scale == 'linear':\n",
    "        if y_max<0: #or True:\n",
    "            pass\n",
    "        else:\n",
    "            ax.set_ylim(0, 1 if shape else 1.2*y_max)\n",
    "    else:\n",
    "        if y_max<0 and not shape:\n",
    "            pass\n",
    "        else:\n",
    "            ax.set_ylim(0.000005 if shape else 0.05, 3 if shape else 300*y_max)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_labels = []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        #print (handle, label)\n",
    "        try:\n",
    "            new_labels.append(my_labels[label])\n",
    "            if not label=='pseudodata':\n",
    "                handle.set_color(colors[label])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if rax:\n",
    "        plt.subplots_adjust(hspace=0)\n",
    "        rax.set_ylabel('Obs./Pred.')\n",
    "        rax.set_ylim(0.5,1.5)\n",
    "\n",
    "    ax.legend(title='',ncol=2,handles=handles, labels=new_labels, frameon=False)\n",
    "\n",
    "    fig.text(0., 0.995, '$\\\\bf{CMS}$', fontsize=20,  horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes )\n",
    "    fig.text(0.15, 1., '$\\\\it{Simulation}$', fontsize=14, horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes )\n",
    "    fig.text(0.8, 1., '13 TeV', fontsize=14, horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes )\n",
    "\n",
    "    fig.savefig(os.path.join(outdir, \"{}.pdf\".format(name)))\n",
    "    fig.savefig(os.path.join(outdir, \"{}.png\".format(name)))\n",
    "    #ax.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histos I want to save\n",
    "histos = [[\"Z_mass\", \"Z_mass_coarse\"]]\n",
    "# histos = [[\"met_CR\", \"MET_ptCoarse\"],\n",
    "#           [\"met_W_CR\", \"MET_ptCoarse\"],\n",
    "#           [\"met_Higgs_CR\", \"MET_ptCoarse\"],\n",
    "#           [\"met_Higgs_W_CR\", \"MET_ptCoarse\"],\n",
    "          \n",
    "#           [\"ht_CR\", \"HT_Coarse\"],\n",
    "#           [\"ht_W_CR\", \"HT_Coarse\"],\n",
    "#           [\"ht_Higgs_CR\", \"HT_Coarse\"],\n",
    "#           [\"ht_Higgs_W_CR\", \"HT_Coarse\"],\n",
    "          \n",
    "#           [\"N_AK8_CR\", \"N_AK8\"],\n",
    "#           [\"N_AK8_W_CR\", \"N_AK8\"],\n",
    "#           [\"N_AK8_Higgs_CR\", \"N_AK8\"],\n",
    "#           [\"N_AK8_Higgs_W_CR\", \"N_AK8\"],\n",
    "          \n",
    "#           [\"W_pt_W_CR\", \"W_pt\"],\n",
    "#           #[\"H_pt_W_CR\", \"H_pt\"],\n",
    "#           [\"W_eta_W_CR\", \"W_eta\"],\n",
    "#           #[\"H_eta_W_CR\", \"H_eta\"],\n",
    "          \n",
    "#           #[\"W_pt_Higgs_CR\", \"W_pt\"],\n",
    "#           [\"H_pt_Higgs_CR\", \"H_pt\"],\n",
    "#           #[\"W_eta_Higgs_CR\", \"W_eta\"],\n",
    "#           [\"H_eta_Higgs_CR\", \"H_eta\"],\n",
    "          \n",
    "#           [\"W_pt_Higgs_W_CR\", \"W_pt\"],\n",
    "#           [\"H_pt_Higgs_W_CR\", \"H_pt\"],\n",
    "#           [\"W_eta_Higgs_W_CR\", \"W_eta\"],\n",
    "#           [\"H_eta_Higgs_W_CR\", \"H_eta\"],\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting aesthetics\n",
    "\n",
    "lineopts = {\n",
    "    'color': 'r',\n",
    "    'linewidth': '3'}\n",
    "\n",
    "data_err_opts = {\n",
    "    'linestyle': 'none',\n",
    "    'marker': '_',\n",
    "    'markersize': 10.,\n",
    "    'color': 'r',\n",
    "    'elinewidth': 1}\n",
    "\n",
    "data_err_opts_rat = {\n",
    "    'linestyle': 'none',\n",
    "    'marker': '.',\n",
    "    'markersize': 10.,\n",
    "    'color': 'k',\n",
    "    'elinewidth': 1}\n",
    "\n",
    "fillopts2 = {\n",
    "    'edgecolor': (0,0,0,0.3),\n",
    "    'facecolor': [('#989C94'),('#6A0136'),('#FF5714'),('#FFCA3A')]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = output['Z_mass']\n",
    "ax = hist.plot1d(histogram,overlay=\"dataset\", stack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some of the plots\n",
    "\n",
    "plotDir = '/home/users/mbryson/public_html/dump/WH/dilep/'\n",
    "finalizePlotDir(plotDir)\n",
    "\n",
    "\n",
    "for plot in histos:\n",
    "\n",
    "    name = plot[0]\n",
    "    binName = plot[1]\n",
    "    histogram = output[name]\n",
    "\n",
    "    axis = bins[binName]['axis']\n",
    "    histogram = histogram.rebin(axis, bins[binName]['bins'])\n",
    "\n",
    "    y_max = histogram.sum(\"dataset\").values(overflow='all')[()].max()\n",
    "    y_over = histogram.sum(\"dataset\").values(overflow='all')[()][-1]\n",
    "\n",
    "    import re\n",
    "\n",
    "    bkg = re.compile('(?!Data)')\n",
    "    \n",
    "    background = histogram[bkg]\n",
    "    data = histogram['Data']\n",
    "\n",
    "    #fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "    fig, (ax, rax) = plt.subplots(nrows=2,ncols=1, figsize=(7,7),\n",
    "        gridspec_kw={\"height_ratios\": (3, 1)}, sharex=True)\n",
    "    \n",
    "    # get axes\n",
    "    hist.plot1d(background, overlay=\"dataset\", ax=ax, stack=True, \n",
    "                overflow=bins[binName]['overflow'], clear=False, fill_opts=fillopts2, \n",
    "                error_opts=error_opts, order=['DYJets','TTJets', 'ttW', 'ttZ']) #error_opts??\n",
    "    hist.plot1d(data, overlay=\"dataset\", ax=ax, stack=False, \n",
    "                overflow=bins[binName]['overflow'], error_opts=data_err_opts_rat, \n",
    "                clear=False)\n",
    "\n",
    "    hist.plotratio(num=data.sum('dataset'), denom=background.sum('dataset'), ax=rax,\n",
    "                   error_opts = data_err_opts_rat, denom_fill_opts={}, guide_opts={}, \n",
    "                   unc='num', overflow = 'over')\n",
    "\n",
    "    for l in ['log', 'linear']:\n",
    "        saveFig(fig, ax, rax, plotDir, name, scale=l, shape=False, y_max=y_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Z_mass'].sum('dataset').values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeaEnv",
   "language": "python",
   "name": "coffeaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
